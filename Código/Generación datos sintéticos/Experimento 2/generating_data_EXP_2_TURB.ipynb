{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM - Big Data Analytics\n",
    "\n",
    "## Generación datos sintéticos - Experimento 2 (Turbidez)\n",
    "\n",
    "### Virginia Casino Sánchez (vcassan@disca.upv.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpL_IpU6jydx"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89neUPbGjwOC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgI-Dmv-jsFy"
   },
   "source": [
    "## Parámetros a configurar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRn4COcx7I7S"
   },
   "outputs": [],
   "source": [
    "exp = 2\n",
    "prueba = 7\n",
    "\n",
    "scaler_on = False\n",
    "scaler_type = MaxAbsScaler()\n",
    "# MaxAbsScaler()\n",
    "# StandardScaler()\n",
    "# MinMaxScaler()\n",
    "# RobustScaler()\n",
    "\n",
    "noise_dim = 6\n",
    "feature_dim = 1\n",
    "time_steps = 6\n",
    "\n",
    "epochs= 500\n",
    "batch_size= 12\n",
    "\n",
    "modelo = 'LSTM_Dropout'\n",
    "# 'Basic'\n",
    "# 'Complex'\n",
    "# 'LeakyRELU'\n",
    "# 'Dropout'\n",
    "# 'LSTM'\n",
    "# 'GRU'\n",
    "# 'LeakyRELU_LSTM'\n",
    "# 'LSTM_Dropout'\n",
    "\n",
    "leaky_relu_alpha = 0.2\n",
    "leaky_relu_negative_slope = 0.2\n",
    "dropout= 0.3\n",
    "\n",
    "lr = 0.0001\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "# tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "# tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "# tf.keras.optimizers.RMSprop(learning_rate=0.00001) #0.00005\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "# 'binary_crossentropy'\n",
    "# 'wasserstein'\n",
    "# 'weighted_loss'\n",
    "\n",
    "#weighted_loss\n",
    "threshold = 10.0\n",
    "factor = 2.0\n",
    "\n",
    "# Establecer la semilla para reproducibilidad\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtgmJ1tFMIRo"
   },
   "outputs": [],
   "source": [
    "if loss != 'weighted_loss':\n",
    "    threshold = np.NaN\n",
    "    factor = np.NaN\n",
    "\n",
    "if not scaler_on:\n",
    "    scaler_type = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WsjbbFNj2YH"
   },
   "source": [
    "## Carga y preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PISoD31rjueU",
    "outputId": "95a7f3d1-eef4-4acb-b4a0-3bbeffc3a72b"
   },
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv('data_turb.csv')\n",
    "\n",
    "# Asegurar que la columna 'date' está en formato datetime y generar las fechas faltantes\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Crear un rango de fechas diarias desde la primera hasta la última fecha del dataset\n",
    "full_date_range = pd.date_range(start=data['date'].min(), end=data['date'].max(), freq='D')\n",
    "\n",
    "# Reindexar los datos para incluir todas las fechas en el rango, asignando NaN a las fechas faltantes\n",
    "data_full = data.set_index('date').reindex(full_date_range).reset_index()\n",
    "data_full.columns = ['date', 'turb']\n",
    "\n",
    "if scaler_on:\n",
    "  # Escalar los valores de 'turb' usando la normalización especificada\n",
    "  scaler = scaler_type\n",
    "  # Escalar solo los valores no nulos\n",
    "  data_full['turb_scaled'] = scaler.fit_transform(data_full[['turb']])\n",
    "else:\n",
    "  # No escalar los valores\n",
    "  data_full['turb_scaled'] = data_full[['turb']]\n",
    "\n",
    "# Reemplazar los NaN por 0 en los datos escalados, pero mantenerlos marcados para luego rellenarlos\n",
    "nan_mask = data_full['turb_scaled'].isna()\n",
    "data_full['turb_scaled'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgOTMx6Smqzc"
   },
   "source": [
    "## Definir arquitectura y entrenamiento de la GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avJjM-DiMIRq"
   },
   "outputs": [],
   "source": [
    "# Definir el generador\n",
    "def build_generator(modelo,\n",
    "                    noise_dim,\n",
    "                    feature_dim,\n",
    "                    time_steps,\n",
    "                    leaky_relu_alpha,\n",
    "                    leaky_relu_negative_slope,\n",
    "                    dropout):\n",
    "\n",
    "    if modelo == 'Basic':\n",
    "      # Modelo básico\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Primera capa densa\n",
    "      model.add(layers.Dense(128, activation='relu', input_dim=noise_dim))\n",
    "\n",
    "      # Segunda capa\n",
    "      model.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "\n",
    "    elif modelo == 'Complex':\n",
    "      # Modelo complejo\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Primera capa\n",
    "      model.add(layers.Dense(256, activation='relu', input_dim=noise_dim))\n",
    "\n",
    "      # Segunda capa con más neuronas\n",
    "      model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "      # Añadir una tercera capa\n",
    "      model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "\n",
    "    elif modelo == 'LeakyRELU':\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(layers.Dense(128, input_dim=noise_dim))\n",
    "      model.add(layers.LeakyReLU(alpha=leaky_relu_alpha))  # Leaky ReLU permite pequeñas pendientes negativas\n",
    "\n",
    "      # Aumentar la complejidad del modelo\n",
    "      model.add(layers.Dense(256))\n",
    "      model.add(layers.LeakyReLU(alpha=leaky_relu_alpha))\n",
    "\n",
    "      # Más capas densas\n",
    "      model.add(layers.Dense(512))\n",
    "      model.add(layers.LeakyReLU(alpha=leaky_relu_alpha))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "\n",
    "    elif modelo == 'Dropout':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Primera capa con Dropout\n",
    "      model.add(layers.Dense(256, activation='relu', input_dim=noise_dim))\n",
    "      model.add(layers.Dropout(dropout))  # Dropout con una tasa del 30%\n",
    "\n",
    "      # Segunda capa con más neuronas y Dropout\n",
    "      model.add(layers.Dense(512, activation='relu'))\n",
    "      model.add(layers.Dropout(dropout))  # Otro Dropout\n",
    "\n",
    "      # Tercera capa\n",
    "      model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))  # Output con activación 'tanh'\n",
    "\n",
    "    elif modelo == 'LSTM':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Expandir la dimensión del ruido para que coincida con (batch_size, time_steps, feature_dim)\n",
    "      model.add(layers.Dense(time_steps * feature_dim, input_dim=noise_dim))\n",
    "      model.add(layers.Reshape((time_steps, feature_dim)))\n",
    "\n",
    "      # Añadir la capa LSTM\n",
    "      model.add(layers.LSTM(256, return_sequences=False))\n",
    "      model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "\n",
    "    elif modelo == 'GRU':\n",
    "      model = tf.keras.Sequential()\n",
    "      # Expandir la dimensión del ruido para que coincida con (batch_size, time_steps, feature_dim)\n",
    "      model.add(layers.Dense(time_steps * feature_dim, input_dim=noise_dim))\n",
    "      model.add(layers.Reshape((time_steps, feature_dim)))\n",
    "\n",
    "      # Cambiar la capa Dense a una capa GRU\n",
    "      model.add(layers.GRU(256, return_sequences=False, input_shape=(time_steps, feature_dim)))\n",
    "      model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "\n",
    "    elif modelo == 'LSTM_Dropout':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Capa densa inicial que convierte el ruido en una forma adecuada\n",
    "      model.add(layers.Dense(time_steps * feature_dim, input_dim=noise_dim))\n",
    "      model.add(layers.Reshape((time_steps, feature_dim)))  # Reformatear a 3D para LSTM\n",
    "\n",
    "      # Capas LSTM con Dropout\n",
    "      model.add(layers.LSTM(256, return_sequences=True))\n",
    "      model.add(layers.Dropout(dropout))  # Aplicar Dropout del 30%\n",
    "\n",
    "      model.add(layers.LSTM(128, return_sequences=False))\n",
    "      model.add(layers.Dropout(dropout))  # Otro Dropout\n",
    "\n",
    "      # Capa densa para generar salida\n",
    "      model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "      # Capa de salida\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "\n",
    "    elif modelo == 'LeakyRELU_LSTM':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Capa inicial para expandir el ruido a la forma (time_steps, feature_dim)\n",
    "      model.add(layers.Dense(time_steps * feature_dim, input_dim=noise_dim))\n",
    "      model.add(layers.Reshape((time_steps, feature_dim)))\n",
    "\n",
    "      # Capa LSTM\n",
    "      model.add(layers.LSTM(256, return_sequences=True))\n",
    "      model.add(layers.LeakyReLU(negative_slope=leaky_relu_negative_slope))\n",
    "\n",
    "      # Segunda capa LSTM o Dense\n",
    "      model.add(layers.LSTM(128, return_sequences=False))\n",
    "      model.add(layers.LeakyReLU(negative_slope=leaky_relu_negative_slope))\n",
    "\n",
    "      # Capa de salida para generar secuencias\n",
    "      model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozMnuNRNMIRr"
   },
   "outputs": [],
   "source": [
    "# Definir el discriminador\n",
    "def build_discriminator(modelo,\n",
    "                        feature_dim,\n",
    "                        time_steps,\n",
    "                        leaky_relu_alpha,\n",
    "                        leaky_relu_negative_slope,\n",
    "                        dropout):\n",
    "    if modelo == 'Basic':\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(layers.Dense(128, activation='relu', input_dim=feature_dim))  # El discriminador recibe un valor continuo\n",
    "      model.add(layers.Dense(256, activation='relu'))\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "\n",
    "    elif modelo == 'Complex':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Aumentar la complejidad con capas adicionales\n",
    "      model.add(layers.Dense(512, input_dim=feature_dim, activation='relu'))\n",
    "      model.add(layers.Dense(256, activation='relu'))\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "      # Capa de salida para clasificación\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    elif modelo == 'LeakyRELU':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Usar LeakyReLU en lugar de ReLU\n",
    "      model.add(layers.Dense(512, input_dim=feature_dim))\n",
    "      model.add(layers.LeakyReLU(alpha=leaky_relu_alpha))\n",
    "\n",
    "      model.add(layers.Dense(256))\n",
    "      model.add(layers.LeakyReLU(alpha=leaky_relu_alpha))\n",
    "\n",
    "      model.add(layers.Dense(128))\n",
    "      model.add(layers.LeakyReLU(alpha=leaky_relu_alpha))\n",
    "\n",
    "      # Capa de salida para clasificación\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    elif modelo == 'Dropout':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Primera capa con Dropout\n",
    "      model.add(layers.Dense(512, input_dim=feature_dim, activation='relu'))\n",
    "      model.add(layers.Dropout(dropout))  # Dropout con una tasa del 30%\n",
    "\n",
    "      # Segunda capa con más neuronas y Dropout\n",
    "      model.add(layers.Dense(256, activation='relu'))\n",
    "      model.add(layers.Dropout(dropout))\n",
    "\n",
    "      # Tercera capa\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "      # Capa de salida para clasificación\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    elif modelo == 'LSTM':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Capa LSTM\n",
    "      model.add(layers.LSTM(256, return_sequences=False, input_shape=(time_steps, feature_dim)))\n",
    "\n",
    "      # Capa densa\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "      # Capa de salida para clasificación\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    elif modelo == 'GRU':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Capa GRU\n",
    "      model.add(layers.GRU(256, return_sequences=False, input_shape=(time_steps, feature_dim)))\n",
    "\n",
    "      # Capa densa\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "      # Capa de salida para clasificación\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    elif modelo == 'LSTM_Dropout':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Capa LSTM con Dropout\n",
    "      model.add(layers.Input(shape=(time_steps, feature_dim)))  # Definir la forma de la entrada\n",
    "      model.add(layers.LSTM(256, return_sequences=True))\n",
    "      model.add(layers.Dropout(dropout))  # Aplicar Dropout del 30%\n",
    "\n",
    "      # Otra capa LSTM\n",
    "      model.add(layers.LSTM(128, return_sequences=False))\n",
    "      model.add(layers.Dropout(dropout))  # Otro Dropout\n",
    "\n",
    "      # Capa densa\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "      # Capa de salida binaria\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))  # Clasificación binaria (real o falso)\n",
    "    elif modelo == 'LeakyRELU_LSTM':\n",
    "      model = tf.keras.Sequential()\n",
    "\n",
    "      # Capa LSTM\n",
    "      model.add(layers.LSTM(256, return_sequences=True, input_shape=(time_steps, feature_dim)))\n",
    "      model.add(layers.LeakyReLU(negative_slope=leaky_relu_negative_slope))\n",
    "\n",
    "      # Capa adicional de LSTM o Dense (si deseas más capas antes de la salida final)\n",
    "      model.add(layers.LSTM(128, return_sequences=False))\n",
    "      model.add(layers.LeakyReLU(negative_slope=leaky_relu_negative_slope))\n",
    "\n",
    "      # Capa de salida para la clasificación binaria (real o falso)\n",
    "      model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4__KEL8MIRs"
   },
   "outputs": [],
   "source": [
    "# Definir el entrenamiento\n",
    "def train_gan(generator, discriminator, gan, data, mean, std, epochs, batch_size, lstm_mode):\n",
    "    for epoch in range(epochs):\n",
    "        if lstm_mode:\n",
    "          # Seleccionar un batch de datos reales (con las dimensiones correctas)\n",
    "          idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "          real_data = data[idx]  # Seleccionar las secuencias reales (batch de secuencias)\n",
    "\n",
    "          # Generar ruido para el generador\n",
    "          noise = np.random.normal(mean, std, (batch_size, generator.input_shape[-1]))\n",
    "\n",
    "        else:\n",
    "          # Seleccionar valores reales aleatorios de los datos\n",
    "          real_data = np.random.choice(data, batch_size)\n",
    "          # Generar ruido y valores falsos\n",
    "          noise = np.random.normal(mean, std, (batch_size, noise_dim))\n",
    "\n",
    "        # Generar datos falsos (generados) a partir del ruido\n",
    "        generated_data = generator.predict(noise)\n",
    "\n",
    "        # Entrenar el discriminador en datos reales y falsos\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        # Entrenar discriminador en datos reales y falsos\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)  # 1 para reales\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)  # 0 para falsos\n",
    "\n",
    "        # Entrenamiento del generador a través del GAN completo\n",
    "        g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "        # Monitoreo de progreso en intervalos de epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - D Loss Real: {d_loss_real} - D Loss Fake: {d_loss_fake} - G Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyvBdvhvmsqE"
   },
   "outputs": [],
   "source": [
    "# Generar datos sintéticos\n",
    "def fill_missing_values(generator, nan_mask, mean, std):\n",
    "    # Crear ruido aleatorio para rellenar los valores faltantes\n",
    "    missing_count = nan_mask.sum()\n",
    "\n",
    "    noise = np.random.normal(mean, std, (missing_count, noise_dim))\n",
    "\n",
    "    # Usar el generador para predecir los valores para las fechas faltantes\n",
    "    generated_values = generator.predict(noise)\n",
    "\n",
    "    return generated_values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFVChuteQi1S"
   },
   "outputs": [],
   "source": [
    "# Definir la pérdida de Wasserstein\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    # Convertimos la condición booleana a float32\n",
    "    weight = 1 + K.cast(y_true > threshold, K.floatx()) * factor\n",
    "\n",
    "    # Calculamos la pérdida ponderada\n",
    "    return K.mean(weight * K.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB43FWp_MIRu"
   },
   "outputs": [],
   "source": [
    "# Crear las secuencias de datos\n",
    "def create_sequences(data, time_steps):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        # Cada secuencia tendrá time_steps de largo\n",
    "        seq = data[i:i + time_steps]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtOaC4XWJNAw",
    "outputId": "dd8a499d-f48c-4c14-d75e-981bb7842e60"
   },
   "outputs": [],
   "source": [
    "# Crear los modelos\n",
    "generator = build_generator(modelo, noise_dim, feature_dim, time_steps, leaky_relu_alpha,leaky_relu_negative_slope, dropout)\n",
    "discriminator = build_discriminator(modelo, feature_dim, time_steps, leaky_relu_alpha,leaky_relu_negative_slope, dropout)\n",
    "\n",
    "# Compilar los modelos\n",
    "if loss == 'wasserstein_loss':\n",
    "  discriminator.compile(optimizer=opt, loss=wasserstein_loss, metrics=['accuracy'])\n",
    "elif loss == 'weighted_loss':\n",
    "  discriminator.compile(optimizer=opt, loss=weighted_loss, metrics=['accuracy'])\n",
    "else:\n",
    "  discriminator.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# El GAN es una combinación de los dos modelos, donde solo se entrena el generador\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Entrada para el GAN: ruido aleatorio\n",
    "gan_input = layers.Input(shape=(noise_dim,))\n",
    "generated_value = generator(gan_input)\n",
    "gan_output = discriminator(generated_value)\n",
    "\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n",
    "if loss == 'wasserstein_loss':\n",
    "  gan.compile(optimizer=opt, loss=wasserstein_loss)\n",
    "elif loss == 'weighted_loss':\n",
    "  gan.compile(optimizer=opt, loss=weighted_loss)\n",
    "else:\n",
    "  gan.compile(optimizer=opt, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "id": "5BdFsoFpJP9a",
    "outputId": "80c61de6-442c-434d-e4d9-c31ed301345c"
   },
   "outputs": [],
   "source": [
    "# Generar el diagrama del generador\n",
    "plot_model(generator, to_file=f'generator_exp{exp}_prueba{prueba}.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# Generar el diagrama del discriminador\n",
    "plot_model(discriminator, to_file=f'discriminator_exp{exp}_prueba{prueba}.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# Generar el diagrama del generador\n",
    "plot_model(gan, to_file=f'gan_exp{exp}_prueba{prueba}.png', show_shapes=True, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqacBU8VcXc1",
    "outputId": "b0371714-177c-46eb-8d2c-632993b75649"
   },
   "outputs": [],
   "source": [
    "# Entrenar el GAN\n",
    "turb_mean = data_full['turb'][~nan_mask].mean()\n",
    "turb_std = data_full['turb'][~nan_mask].std()\n",
    "\n",
    "if modelo == 'LSTM' or modelo == 'GRU' or modelo == 'LSTM_Dropout' or modelo == 'LeakyRELU_LSTM':\n",
    "  lstm_mode = True\n",
    "  # Usar esta función para transformar los datos de entrenamiento\n",
    "  data_sequences = create_sequences(data_full['turb_scaled'][~nan_mask].values, time_steps)\n",
    "\n",
    "  # Asegúrate de que los datos tengan la forma correcta: (n_samples, time_steps, feature_dim)\n",
    "  data_sequences = data_sequences.reshape(-1, time_steps, feature_dim)\n",
    "\n",
    "  # Ejecutar el entrenamiento (usamos los valores escalados que no son NaN)\n",
    "  train_gan(generator, discriminator, gan, data_sequences, turb_mean, turb_std, epochs, batch_size, lstm_mode)\n",
    "else:\n",
    "  lstm_mode = False\n",
    "  # Ejecutar el entrenamiento (usamos los valores escalados que no son NaN)\n",
    "  train_gan(generator, discriminator, gan, data_full['turb_scaled'][~nan_mask].values, turb_mean, turb_std, epochs, batch_size, lstm_mode)\n",
    "\n",
    "# Utilizar el generador entrenado para predecir los valores faltantes\n",
    "generated_values_scaled = fill_missing_values(generator, nan_mask, turb_mean, turb_std)\n",
    "\n",
    "# Reemplazar los valores faltantes escalados por los generados\n",
    "data_full.loc[nan_mask, 'turb_scaled'] = generated_values_scaled\n",
    "\n",
    "if scaler_on:\n",
    "  # Reescalar los valores generados a su rango original\n",
    "  data_full['turb_filled'] = scaler.inverse_transform(data_full[['turb_scaled']])\n",
    "else:\n",
    "  data_full['turb_filled'] = data_full[['turb_scaled']]\n",
    "\n",
    "data_full['turb_new'] = data_full['turb_filled']\n",
    "data_full.loc[data_full['turb'].notna(), 'turb_new'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFtjwgDuoxsB"
   },
   "source": [
    "## Visualizar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6fEr8LpMIRx"
   },
   "outputs": [],
   "source": [
    "def pintar_grafica(titulo, file_svg, file_png, type):\n",
    "    ancho = 15\n",
    "    alto = 10\n",
    "    label_x = 'Fecha'\n",
    "    label_y = 'Turbidez'\n",
    "\n",
    "    # Preparación gráfica\n",
    "    fig, ax = plt.subplots(figsize=(ancho, alto))\n",
    "\n",
    "    if type == 'scatter':\n",
    "        # Crear el gráfico de scatter\n",
    "        plt.scatter(data_full['date'], data_full['turb'], label='Entrenamiento', color='royalblue', marker='o')\n",
    "        plt.scatter(data_full['date'], data_full['turb_new'], label='Generados (reconstrucción)', color='seagreen', marker='x')\n",
    "    elif type == 'unificado':\n",
    "      # Crear el gráfico todos los valores unificados\n",
    "        plt.plot(data_full['date'], data_full['turb_filled'], color='royalblue')\n",
    "    elif type == 'histograma':\n",
    "      # Crear el histograma\n",
    "      plt.hist([data_full['turb'], data_full['turb_new']],\n",
    "               label=[\"Real\", \"Sintético\"],\n",
    "               bins=25,\n",
    "               density=True, color=['royalblue','seagreen'])\n",
    "    else:\n",
    "        # Crear el gráfico de líneas\n",
    "        plt.plot(data_full['date'], data_full['turb'], label='Entrenamiento', color='royalblue')\n",
    "        plt.plot(data_full['date'], data_full['turb_new'], label='Generados (reconstrucción)', linestyle='--', color='seagreen')\n",
    "\n",
    "    # Configuración a los ejes / grid\n",
    "        # Color de fondo\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "        # Configuración del grid\n",
    "    ax.grid(True, color='lightgrey', linestyle='--', linewidth=0.7, axis='y', which='major')\n",
    "    ax.yaxis.grid(True, color='lightgrey', linestyle='--', linewidth=0.2, which='minor')\n",
    "\n",
    "        # Configuración de los ticks del eje y\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(10))   # Ticks mayores cada 1\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(50)) # Ticks menores cada 0.5\n",
    "\n",
    "    ax.tick_params(axis='y', labelsize=25)\n",
    "\n",
    "        # Configuración de los ticks del eje x\n",
    "    ax.tick_params(axis='x', which='major', labelsize=25)\n",
    "\n",
    "    # Leyenda\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    # Configuración de las etiquetas\n",
    "    ax.set_xlabel(label_x, fontsize=30, labelpad=25)\n",
    "    ax.set_ylabel(label_y, fontsize=30, labelpad=25)\n",
    "    ax.set_title(titulo, fontsize=35, pad=10)\n",
    "\n",
    "    # Borrar líneas superior y derecha\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Ajustar imagen\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar imagen\n",
    "    plt.savefig(file_svg, format='svg')\n",
    "    plt.savefig(file_png, format='png')\n",
    "\n",
    "    # Mostrar imagen\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "HeZ7tKMVtgQn",
    "outputId": "0ee0cba0-c03c-4add-f51b-dacf75f2c61d"
   },
   "outputs": [],
   "source": [
    "titulo = f'Scatter Experimento {exp} Prueba {prueba}'\n",
    "file_svg_scatter = f'scatter_{exp}_{prueba}.svg'\n",
    "file_png_scatter = f'scater_{exp}_{prueba}.png'\n",
    "type = 'scatter'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_scatter, file_png_scatter, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ssl2F6SXs_Pr",
    "outputId": "cf5603e9-270a-4b33-a755-44bccfe83352"
   },
   "outputs": [],
   "source": [
    "\n",
    "titulo = f'Resultados Experimento {exp} Prueba {prueba}'\n",
    "file_svg_res = f'resultado_{exp}_{prueba}.svg'\n",
    "file_png_res = f'resultado_{exp}_{prueba}.png'\n",
    "\n",
    "type = 'line'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_res, file_png_res, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qDtEgRZzMIRy",
    "outputId": "b5533995-1586-4eee-9929-18bc4cb07ff3"
   },
   "outputs": [],
   "source": [
    "titulo = f'Unificado Experimento {exp} Prueba {prueba}'\n",
    "file_svg_union = f'union_{exp}_{prueba}.svg'\n",
    "file_png_union = f'union_{exp}_{prueba}.png'\n",
    "\n",
    "type = 'unificado'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_union, file_png_union, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Im9ascKIMIRz",
    "outputId": "e0e2bf6e-af6c-4af3-c2d0-75766c1864f3"
   },
   "outputs": [],
   "source": [
    "titulo = f'Histograma Experimento {exp} Prueba {prueba}'\n",
    "file_svg_histo = f'histo_{exp}_{prueba}.svg'\n",
    "file_png_histo = f'histo_{exp}_{prueba}.png'\n",
    "\n",
    "type = 'histograma'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_histo, file_png_histo, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-PJOeP0ovww"
   },
   "source": [
    "## Obtener resultados numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0k41-0RbVrP",
    "outputId": "017e1bda-e41b-4178-d45e-c1794039b4b2"
   },
   "outputs": [],
   "source": [
    "# Extraer los valores reales y generados del dataset\n",
    "real_values = data_full['turb'].dropna().values\n",
    "generated_values = data_full['turb_filled'].dropna().values\n",
    "\n",
    "\n",
    "# Funciones de evaluación para comparar datos reales y generados\n",
    "\n",
    "# Distancia de Jensen-Shannon: mide la diferencia entre distribuciones\n",
    "def calculate_jsd(real_data, generated_data, bins=50):\n",
    "    # Obtener el rango común para ambos conjuntos de datos\n",
    "    data_min = min(real_data.min(), generated_data.min())\n",
    "    data_max = max(real_data.max(), generated_data.max())\n",
    "\n",
    "    # Crear bin edges basados en este rango común\n",
    "    bins = np.linspace(data_min, data_max, 100)  # Número de bins configurable\n",
    "\n",
    "    # Generar histogramas usando los mismos bin edges\n",
    "    # Histograma de los datos reales y generados\n",
    "    real_hist, bin_edges = np.histogram(real_data, bins=bins, density=True)\n",
    "    generated_hist, _ = np.histogram(generated_data, bins=bins, density=True)\n",
    "\n",
    "    # Normalizar las distribuciones para que sumen 1\n",
    "    real_hist = real_hist / np.sum(real_hist)\n",
    "    generated_hist = generated_hist / np.sum(generated_hist)\n",
    "\n",
    "    # Calcular la distancia de Jensen-Shannon\n",
    "    jsd = jensenshannon(real_hist, generated_hist, base=2)\n",
    "    return jsd\n",
    "\n",
    "def calculate_wd(real_data, generated_data):\n",
    "  return wasserstein_distance(real_data, generated_data)\n",
    "\n",
    "def calculate_ks(real_data, generated_data):\n",
    "  return ks_2samp(real_data, generated_data)\n",
    "\n",
    "# Calcular la distancia de Jensen-Shannon\n",
    "jsd_value = calculate_jsd(real_values, generated_values)\n",
    "print(f\"Distancia de Jensen-Shannon: {jsd_value}\")\n",
    "print()\n",
    "\n",
    "# Calcular la distancia de Wasserstein\n",
    "wasserstein_dist = calculate_wd(real_values, generated_values)\n",
    "print(f\"Wasserstein Distance: {wasserstein_dist}\")\n",
    "print()\n",
    "\n",
    "# Realizar el Kolmogorov-Smirnov Test\n",
    "ks_stat, p_value = calculate_ks(real_values, generated_values)\n",
    "print(f\"K-S Statistic: {ks_stat}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHToCbrIr9VD"
   },
   "source": [
    "## Guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ctt04f6Or_MD",
    "outputId": "2d57531f-3153-4952-e672-aaa0804b7e44"
   },
   "outputs": [],
   "source": [
    "file_models = f'model_summaries_{exp}_{prueba}.txt'\n",
    "\n",
    "with open(file_models, 'w', encoding='utf-8') as f:\n",
    "    # Guardar resumen del generador\n",
    "    f.write(\"Resumen del Generador:\\n\")\n",
    "    generator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    # Guardar resumen del discriminador\n",
    "    f.write(\"\\nResumen del Discriminador:\\n\")\n",
    "    discriminator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    # Guardar resumen de la GAN\n",
    "    f.write(\"\\nResumen de la GAN:\\n\")\n",
    "    gan.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0L33eoHFYfV"
   },
   "outputs": [],
   "source": [
    "conf_res_data = {\n",
    "    'experimento': [exp],\n",
    "    'prueba': [prueba],\n",
    "    'normalizado': [scaler_on],\n",
    "    'tipo_normalizacion': [scaler_type],\n",
    "    'dim_ruido': [noise_dim],\n",
    "    'feature_dim': [feature_dim],\n",
    "    'timesteps': [time_steps],\n",
    "    'epochs': [epochs],\n",
    "    'batch_size': [batch_size],\n",
    "    'model': [modelo],\n",
    "    'alpha': [leaky_relu_alpha],\n",
    "    'negative_slope': [leaky_relu_negative_slope],\n",
    "    'dropuot': [dropout],\n",
    "    'optimizador': [opt],\n",
    "    'learning_rate':[lr],\n",
    "    'loss': [loss],\n",
    "    'weighted_thr': [threshold],\n",
    "    'weighted_factor': [factor],\n",
    "    'jensen_shannon': [jsd_value],\n",
    "    'wasserstein_distance': [wasserstein_dist],\n",
    "    'kolmogorov_smirnov': [ks_stat],\n",
    "    'p_value': [p_value]\n",
    "}\n",
    "\n",
    "conf_res= pd.DataFrame(conf_res_data)\n",
    "\n",
    "file_conf_res = f'config_results_{exp}_{prueba}.csv'\n",
    "\n",
    "conf_res.to_csv(file_conf_res, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WogSICilI5dc"
   },
   "outputs": [],
   "source": [
    "file_turb_data= f'turb_data_{exp}_{prueba}.csv'\n",
    "data_full[['date','turb_filled']].to_csv(file_turb_data, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
