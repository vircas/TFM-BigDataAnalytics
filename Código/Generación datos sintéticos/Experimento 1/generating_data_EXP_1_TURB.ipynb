{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM - Big Data Analytics\n",
    "\n",
    "## Generación datos sintéticos - Experimento 1 (Turbidez)\n",
    "\n",
    "### Virginia Casino Sánchez (vcassan@disca.upv.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpL_IpU6jydx"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89neUPbGjwOC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import ks_2samp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgI-Dmv-jsFy"
   },
   "source": [
    "## Parámetros a configurar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRn4COcx7I7S"
   },
   "outputs": [],
   "source": [
    "exp = 1\n",
    "prueba = 36\n",
    "\n",
    "scaler_on = True\n",
    "scaler_type = MaxAbsScaler()\n",
    "\n",
    "# MaxAbsScaler()\n",
    "# StandardScaler()\n",
    "# MinMaxScaler()\n",
    "# RobustScaler()\n",
    "\n",
    "noise_dim = 6\n",
    "feature_dim = 1\n",
    "time_steps = 6 #100 #30\n",
    "\n",
    "epochs= 1000 #1000\n",
    "batch_size= 6\n",
    "\n",
    "lr = 0.00001\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "# tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "# tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "# tf.keras.optimizers.RMSprop(learning_rate=0.00001) #0.00005\n",
    "\n",
    "loss = 'weighted_loss'\n",
    "# 'binary_crossentropy'\n",
    "# 'wasserstein_loss'\n",
    "# 'weighted_loss'\n",
    "\n",
    "#weighted_loss\n",
    "threshold = 10.0\n",
    "factor = 50.0\n",
    "\n",
    "# Establecer la semilla para reproducibilidad\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lhb1KcqHUelc"
   },
   "outputs": [],
   "source": [
    "if loss != 'weighted_loss':\n",
    "    threshold = np.NaN\n",
    "    factor = np.NaN\n",
    "\n",
    "if not scaler_on:\n",
    "    scaler_type = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WsjbbFNj2YH"
   },
   "source": [
    "## Carga y preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PISoD31rjueU",
    "outputId": "d5de3885-20ef-44f3-eca0-fb663a62288b"
   },
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv('data_turb.csv')\n",
    "\n",
    "# Asegurar que la columna 'date' está en formato datetime y generar las fechas faltantes\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Crear un rango de fechas diarias desde la primera hasta la última fecha del dataset\n",
    "full_date_range = pd.date_range(start=data['date'].min(), end=data['date'].max(), freq='D')\n",
    "\n",
    "# Reindexar los datos para incluir todas las fechas en el rango, asignando NaN a las fechas faltantes\n",
    "data_full = data.set_index('date').reindex(full_date_range).reset_index()\n",
    "data_full.columns = ['date', 'turb']\n",
    "\n",
    "if scaler_on:\n",
    "  # Escalar los valores de 'turb' usando la normalización especificada\n",
    "  scaler = scaler_type\n",
    "  # Escalar solo los valores no nulos\n",
    "  data_full['turb_scaled'] = scaler.fit_transform(data_full[['turb']])\n",
    "else:\n",
    "  # No escalar los valores\n",
    "  data_full['turb_scaled'] = data_full[['turb']]\n",
    "\n",
    "# Reemplazar los NaN por 0 en los datos escalados, pero mantenerlos marcados para luego rellenarlos\n",
    "nan_mask = data_full['turb_scaled'].isna()\n",
    "data_full['turb_scaled'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgOTMx6Smqzc"
   },
   "source": [
    "## Definir arquitectura y entrenamiento de la GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4UrmhD6Ueld"
   },
   "outputs": [],
   "source": [
    "# Definir el generador\n",
    "def build_generator(noise_dim,\n",
    "                    feature_dim):\n",
    "    # Modelo básico\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Primera capa densa\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=noise_dim))\n",
    "\n",
    "    # Segunda capa\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(feature_dim, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUYBvivoUeld"
   },
   "outputs": [],
   "source": [
    "# Definir el discriminador\n",
    "def build_discriminator(feature_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=feature_dim))  # El discriminador recibe un valor continuo\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAnoofS9Uele"
   },
   "outputs": [],
   "source": [
    "# Definir el entrenamiento\n",
    "def train_gan(generator, discriminator, gan, data, mean, std, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        # Seleccionar valores reales aleatorios de los datos\n",
    "        real_data = np.random.choice(data, batch_size)\n",
    "        # Generar ruido y valores falsos\n",
    "        noise = np.random.normal(mean, std, (batch_size, noise_dim))\n",
    "\n",
    "        # Generar datos falsos (generados) a partir del ruido\n",
    "        generated_data = generator.predict(noise)\n",
    "\n",
    "        # Entrenar el discriminador en datos reales y falsos\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        # Entrenar discriminador en datos reales y falsos\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)  # 1 para reales\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)  # 0 para falsos\n",
    "\n",
    "        # Entrenamiento del generador a través del GAN completo\n",
    "        g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "        # Monitoreo de progreso en intervalos de epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - D Loss Real: {d_loss_real} - D Loss Fake: {d_loss_fake} - G Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyvBdvhvmsqE"
   },
   "outputs": [],
   "source": [
    "# Generar datos sintéticos\n",
    "def fill_missing_values(generator, nan_mask, mean, std):\n",
    "    # Crear ruido aleatorio para rellenar los valores faltantes\n",
    "    missing_count = nan_mask.sum()\n",
    "\n",
    "    noise = np.random.normal(mean, std, (missing_count, noise_dim))\n",
    "\n",
    "    # Usar el generador para predecir los valores para las fechas faltantes\n",
    "    generated_values = generator.predict(noise)\n",
    "\n",
    "    return generated_values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFVChuteQi1S"
   },
   "outputs": [],
   "source": [
    "# Definir la pérdida de Wasserstein\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    # Convertimos la condición booleana a float32\n",
    "    weight = 1 + K.cast(y_true > threshold, K.floatx()) * factor\n",
    "\n",
    "    # Calculamos la pérdida ponderada\n",
    "    return K.mean(weight * K.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4XnXHKWUele"
   },
   "outputs": [],
   "source": [
    "# Crear las secuencias de datos\n",
    "def create_sequences(data, time_steps):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        # Cada secuencia tendrá time_steps de largo\n",
    "        seq = data[i:i + time_steps]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtOaC4XWJNAw",
    "outputId": "a3a661ff-4854-4090-d207-b28aee09d521"
   },
   "outputs": [],
   "source": [
    "# Crear los modelos\n",
    "generator = build_generator(noise_dim, feature_dim)\n",
    "discriminator = build_discriminator(feature_dim)\n",
    "\n",
    "# Compilar los modelos\n",
    "if loss == 'wasserstein_loss':\n",
    "  discriminator.compile(optimizer=opt, loss=wasserstein_loss, metrics=['accuracy'])\n",
    "elif loss == 'weighted_loss':\n",
    "  discriminator.compile(optimizer=opt, loss=weighted_loss, metrics=['accuracy'])\n",
    "else:\n",
    "  discriminator.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# El GAN es una combinación de los dos modelos, donde solo se entrena el generador\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Entrada para el GAN: ruido aleatorio\n",
    "gan_input = layers.Input(shape=(noise_dim,))\n",
    "generated_value = generator(gan_input)\n",
    "gan_output = discriminator(generated_value)\n",
    "\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n",
    "if loss == 'wasserstein_loss':\n",
    "  gan.compile(optimizer=opt, loss=wasserstein_loss)\n",
    "elif loss == 'weighted_loss':\n",
    "  gan.compile(optimizer=opt, loss=weighted_loss)\n",
    "else:\n",
    "  gan.compile(optimizer=opt, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "5BdFsoFpJP9a",
    "outputId": "f108626f-f195-4673-9474-3de52cdc1dcf"
   },
   "outputs": [],
   "source": [
    "# Generar el diagrama del generador\n",
    "plot_model(generator, to_file=f'generator_exp{exp}_prueba{prueba}.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# Generar el diagrama del discriminador\n",
    "plot_model(discriminator, to_file=f'discriminator_exp{exp}_prueba{prueba}.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "# Generar el diagrama del generador\n",
    "plot_model(gan, to_file=f'gan_exp{exp}_prueba{prueba}.png', show_shapes=True, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqacBU8VcXc1",
    "outputId": "a3691174-9306-4fe9-9d0a-7ea146fa533b"
   },
   "outputs": [],
   "source": [
    "# Entrenar el GAN\n",
    "turb_mean = data_full['turb'][~nan_mask].mean()\n",
    "turb_std = data_full['turb'][~nan_mask].std()\n",
    "\n",
    "# Ejecutar el entrenamiento (usamos los valores escalados que no son NaN)\n",
    "train_gan(generator,\n",
    "          discriminator,\n",
    "          gan,\n",
    "          data_full['turb_scaled'][~nan_mask].values,\n",
    "          turb_mean,\n",
    "          turb_std,\n",
    "          epochs,\n",
    "          batch_size)\n",
    "\n",
    "# Utilizar el generador entrenado para predecir los valores faltantes\n",
    "generated_values_scaled = fill_missing_values(generator, nan_mask, turb_mean, turb_std)\n",
    "\n",
    "# Reemplazar los valores faltantes escalados por los generados\n",
    "data_full.loc[nan_mask, 'turb_scaled'] = generated_values_scaled\n",
    "\n",
    "if scaler_on:\n",
    "  # Reescalar los valores generados a su rango original\n",
    "  data_full['turb_filled'] = scaler.inverse_transform(data_full[['turb_scaled']])\n",
    "else:\n",
    "  data_full['turb_filled'] = data_full[['turb_scaled']]\n",
    "\n",
    "data_full['turb_new'] = data_full['turb_filled']\n",
    "data_full.loc[data_full['turb'].notna(), 'turb_new'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFtjwgDuoxsB"
   },
   "source": [
    "## Visualizar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1Vt7d0iUelf"
   },
   "outputs": [],
   "source": [
    "def pintar_grafica(titulo, file_svg, file_png, type):\n",
    "    ancho = 15\n",
    "    alto = 10\n",
    "    label_x = 'Fecha'\n",
    "    label_y = 'Turbidez'\n",
    "\n",
    "    # Preparación gráfica\n",
    "    fig, ax = plt.subplots(figsize=(ancho, alto))\n",
    "\n",
    "    if type == 'scatter':\n",
    "        # Crear el gráfico de scatter\n",
    "        plt.scatter(data_full['date'], data_full['turb'], label='Entrenamiento', color='royalblue', marker='o')\n",
    "        plt.scatter(data_full['date'], data_full['turb_new'], label='Generados', color='seagreen', marker='x', alpha = 0.4)\n",
    "    elif type == 'unificado':\n",
    "      # Crear el gráfico todos los valores unificados\n",
    "        plt.plot(data_full['date'], data_full['turb_filled'], color='royalblue')\n",
    "    elif type == 'histograma':\n",
    "      # Crear el histograma\n",
    "      plt.hist([data_full['turb'], data_full['turb_new']],\n",
    "               label=[\"Real\", \"Sintético\"],\n",
    "               bins=25,\n",
    "               density=True, color=['royalblue','seagreen'])\n",
    "    else:\n",
    "        # Crear el gráfico de líneas\n",
    "        plt.plot(data_full['date'], data_full['turb'], label='Entrenamiento', color='royalblue')\n",
    "        plt.plot(data_full['date'], data_full['turb_new'], label='Generados', linestyle='--', color='seagreen', alpha = 0.4)\n",
    "\n",
    "    # Configuración a los ejes / grid\n",
    "        # Color de fondo\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "        # Configuración del grid\n",
    "    ax.grid(True, color='lightgrey', linestyle='--', linewidth=0.7, axis='y', which='major')\n",
    "    ax.yaxis.grid(True, color='lightgrey', linestyle='--', linewidth=0.2, which='minor')\n",
    "\n",
    "        # Configuración de los ticks del eje y\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(10))   # Ticks mayores cada 1\n",
    "    ax.yaxis.set_minor_locator(ticker.MultipleLocator(50)) # Ticks menores cada 0.5\n",
    "\n",
    "    ax.tick_params(axis='y', labelsize=25)\n",
    "\n",
    "        # Configuración de los ticks del eje x\n",
    "    ax.tick_params(axis='x', which='major', labelsize=25)\n",
    "\n",
    "    # Leyenda\n",
    "    if type != 'unificado':\n",
    "      ax.legend(fontsize=20)\n",
    "\n",
    "    # Configuración de las etiquetas\n",
    "    ax.set_xlabel(label_x, fontsize=30, labelpad=25)\n",
    "    ax.set_ylabel(label_y, fontsize=30, labelpad=25)\n",
    "    ax.set_title(titulo, fontsize=35, pad=10)\n",
    "\n",
    "    # Borrar líneas superior y derecha\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Ajustar imagen\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar imagen\n",
    "    plt.savefig(file_svg, format='svg')\n",
    "    plt.savefig(file_png, format='png')\n",
    "\n",
    "    # Mostrar imagen\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeZ7tKMVtgQn"
   },
   "outputs": [],
   "source": [
    "titulo = f'Scatter Experimento {exp} Prueba {prueba}'\n",
    "file_svg_scatter = f'scatter_{exp}_{prueba}.svg'\n",
    "file_png_scatter = f'scater_{exp}_{prueba}.png'\n",
    "type = 'scatter'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_scatter, file_png_scatter, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssl2F6SXs_Pr"
   },
   "outputs": [],
   "source": [
    "titulo = f'Resultados Experimento {exp} Prueba {prueba}'\n",
    "file_svg_res = f'resultado_{exp}_{prueba}.svg'\n",
    "file_png_res = f'resultado_{exp}_{prueba}.png'\n",
    "\n",
    "type = 'line'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_res, file_png_res, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7kCwr0cXbjL"
   },
   "outputs": [],
   "source": [
    "titulo = f'Unificado Experimento {exp} Prueba {prueba}'\n",
    "file_svg_union = f'union_{exp}_{prueba}.svg'\n",
    "file_png_union = f'union_{exp}_{prueba}.png'\n",
    "\n",
    "type = 'unificado'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_union, file_png_union, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJqAvy0VY9XV"
   },
   "outputs": [],
   "source": [
    "titulo = f'Histograma Experimento {exp} Prueba {prueba}'\n",
    "file_svg_histo = f'histo_{exp}_{prueba}.svg'\n",
    "file_png_histo = f'histo_{exp}_{prueba}.png'\n",
    "\n",
    "type = 'histograma'\n",
    "\n",
    "pintar_grafica(titulo, file_svg_histo, file_png_histo, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-PJOeP0ovww"
   },
   "source": [
    "## Obtener resultados numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0k41-0RbVrP"
   },
   "outputs": [],
   "source": [
    "# Extraer los valores reales y generados del dataset\n",
    "real_values = data_full['turb'].dropna().values\n",
    "generated_values = data_full['turb_filled'].dropna().values\n",
    "\n",
    "\n",
    "# Funciones de evaluación para comparar datos reales y generados\n",
    "\n",
    "# Distancia de Jensen-Shannon: mide la diferencia entre distribuciones\n",
    "def calculate_jsd(real_data, generated_data, bins=50):\n",
    "    # Obtener el rango común para ambos conjuntos de datos\n",
    "    data_min = min(real_data.min(), generated_data.min())\n",
    "    data_max = max(real_data.max(), generated_data.max())\n",
    "\n",
    "    # Crear bin edges basados en este rango común\n",
    "    bins = np.linspace(data_min, data_max, 100)  # Número de bins configurable\n",
    "\n",
    "    # Generar histogramas usando los mismos bin edges\n",
    "    # Histograma de los datos reales y generados\n",
    "    real_hist, bin_edges = np.histogram(real_data, bins=bins, density=True)\n",
    "    generated_hist, _ = np.histogram(generated_data, bins=bins, density=True)\n",
    "\n",
    "    # Normalizar las distribuciones para que sumen 1\n",
    "    real_hist = real_hist / np.sum(real_hist)\n",
    "    generated_hist = generated_hist / np.sum(generated_hist)\n",
    "\n",
    "    # Calcular la distancia de Jensen-Shannon\n",
    "    jsd = jensenshannon(real_hist, generated_hist, base=2)\n",
    "    return jsd\n",
    "\n",
    "def calculate_wd(real_data, generated_data):\n",
    "  return wasserstein_distance(real_data, generated_data)\n",
    "\n",
    "def calculate_ks(real_data, generated_data):\n",
    "  return ks_2samp(real_data, generated_data)\n",
    "\n",
    "# Calcular la distancia de Jensen-Shannon\n",
    "jsd_value = calculate_jsd(real_values, generated_values)\n",
    "print(f\"Distancia de Jensen-Shannon: {jsd_value}\")\n",
    "print()\n",
    "\n",
    "# Calcular la distancia de Wasserstein\n",
    "wasserstein_dist = calculate_wd(real_values, generated_values)\n",
    "print(f\"Wasserstein Distance: {wasserstein_dist}\")\n",
    "print()\n",
    "\n",
    "# Realizar el Kolmogorov-Smirnov Test\n",
    "ks_stat, p_value = calculate_ks(real_values, generated_values)\n",
    "print(f\"K-S Statistic: {ks_stat}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHToCbrIr9VD"
   },
   "source": [
    "## Guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctt04f6Or_MD"
   },
   "outputs": [],
   "source": [
    "file_models = f'model_summaries_{exp}_{prueba}.txt'\n",
    "\n",
    "with open(file_models, 'w', encoding='utf-8') as f:\n",
    "    # Guardar resumen del generador\n",
    "    f.write(\"Resumen del Generador:\\n\")\n",
    "    generator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    # Guardar resumen del discriminador\n",
    "    f.write(\"\\nResumen del Discriminador:\\n\")\n",
    "    discriminator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    # Guardar resumen de la GAN\n",
    "    f.write(\"\\nResumen de la GAN:\\n\")\n",
    "    gan.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0L33eoHFYfV"
   },
   "outputs": [],
   "source": [
    "conf_res_data = {\n",
    "    'experimento': [exp],\n",
    "    'prueba': [prueba],\n",
    "    'normalizado': [scaler_on],\n",
    "    'tipo_normalizacion': [scaler_type],\n",
    "    'dim_ruido': [noise_dim],\n",
    "    'feature_dim': [feature_dim],\n",
    "    'timesteps': [time_steps],\n",
    "    'epochs': [epochs],\n",
    "    'batch_size': [batch_size],\n",
    "    'optimizador': [opt],\n",
    "    'learning_rate':[lr],\n",
    "    'loss': [loss],\n",
    "    'weighted_thr': [threshold],\n",
    "    'weighted_factor': [factor],\n",
    "    'jensen_shannon': [jsd_value],\n",
    "    'wasserstein_distance': [wasserstein_dist],\n",
    "    'kolmogorov_smirnov': [ks_stat],\n",
    "    'p_value': [p_value]\n",
    "}\n",
    "\n",
    "conf_res= pd.DataFrame(conf_res_data)\n",
    "\n",
    "file_conf_res = f'config_results_{exp}_{prueba}.csv'\n",
    "\n",
    "conf_res.to_csv(file_conf_res, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WogSICilI5dc"
   },
   "outputs": [],
   "source": [
    "file_turb_data= f'turb_data_{exp}_{prueba}.csv'\n",
    "data_full[['date','turb_filled']].to_csv(file_turb_data, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
